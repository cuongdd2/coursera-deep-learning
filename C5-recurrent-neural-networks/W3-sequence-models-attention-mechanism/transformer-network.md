## Intuition
- Published 2017 Vaswani
- Attention + CNN
- Self attention
- Multi-head Attention

- A(q, K, V) = attention based vector representation of a word
- calculate for each word A<1> -> A<5>
- q: query = Wq x to ask question about that word, what happens
- k: key = Wk x  represent the  word
- v: value = Wv x 

## Self attention
## Multi-head attention

Positional encoding using sin, cos