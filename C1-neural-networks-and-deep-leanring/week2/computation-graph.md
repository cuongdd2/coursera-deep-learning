Derivative of computation graph is backward propagation
Logistic Regression Gradient Descent
 - z = wTx + b
 - y^ = a = sigmoid(z)
 - L(a, y) = -(ylog(a) + (1 - y)log(1 - a))